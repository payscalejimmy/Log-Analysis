# -*- coding: utf-8 -*-
"""WIP Payscale Parser

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x0S9XcJ5iULEcM8frDPNpoP1qnSzdCRa

## CELL 1 - CSV Combiner & Data Cleaning (Run Once)
"""

import pandas as pd
import numpy as np
import re
from urllib.parse import urlparse
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import files
from google.colab import drive
from collections import Counter
import warnings
import os
from collections import OrderedDict
from datetime import datetime
warnings.filterwarnings('ignore')

def mount_drive():
    """Mount Google Drive"""
    try:
        drive.mount('/content/drive')
        return True
    except Exception as e:
        print(f"‚ùå Error mounting drive: {str(e)}")
        return False

def list_csvs_in_drive(folder_path):
    """List all CSV files in the specified Google Drive folder"""
    try:
        csv_files = []
        if os.path.exists(folder_path):
            for file in os.listdir(folder_path):
                if file.lower().endswith('.csv'):
                    csv_files.append(os.path.join(folder_path, file))
        return csv_files
    except Exception as e:
        print(f"‚ùå Error listing files: {str(e)}")
        return []

def load_and_combine_csvs():
    """Main function to load and combine CSVs, automatically loading from Google Drive."""
    print("üîÑ CSV Combiner & Cleaner Tool")
    print("=" * 50)

    # Initialize variables
    uploaded_files = {} # Not used, but kept for function logic
    drive_files = []
    dfs = []

    # Handle Google Drive files automatically
    print("\nüîó Connecting to Google Drive...")
    if mount_drive():
        # drive_folder = "/content/drive/MyDrive/log_uploads"
        drive_folder = "/content/drive/MyDrive/botify_logs/sep1-4"
        drive_files = list_csvs_in_drive(drive_folder)

        if drive_files:
            print(f"üìÅ Found {len(drive_files)} CSV files in Google Drive:")
            for i, file_path in enumerate(drive_files, 1):
                filename = os.path.basename(file_path)
                print(f"  {i}. {filename}")
        else:
            print("‚ö†Ô∏è  No CSV files found in /log_uploads directory")

    print(f"\nüßπ Loading and combining data...")
    print("=" * 50)

    # Process Google Drive files
    if drive_files:
        print(f"\n‚òÅÔ∏è  Processing Google Drive files:")
        for file_path in drive_files:
            filename = os.path.basename(file_path)
            print(f"üìÑ Processing: {filename}")
            try:
                temp_df = pd.read_csv(file_path)
                print(f"  - Shape: {temp_df.shape}")
                temp_df['source_file'] = filename
                temp_df['source_type'] = 'google_drive'
                dfs.append(temp_df)
            except Exception as e:
                print(f"  ‚ùå Error reading {filename}: {str(e)}")

    if not dfs:
        print("‚ùå No CSV files were successfully loaded!")
        return None

    # Combine all dataframes
    print(f"\nüîó Combining {len(dfs)} CSV files...")
    df = pd.concat(dfs, ignore_index=True, sort=False)

    # Clean data
    print(f"\nüßπ Cleaning data...")
    df.columns = df.columns.str.strip()

    # Remove duplicates
    initial_count = len(df)
    df = df.drop_duplicates(subset=['Full URL'], keep='first')
    duplicates_removed = initial_count - len(df)

    print(f"‚úÖ Data cleaning complete:")
    print(f"  - Original rows: {initial_count:,}")
    print(f"  - After deduplication: {len(df):,}")
    print(f"  - Duplicates removed: {duplicates_removed:,}")

    # Handle missing values
    numeric_columns = df.select_dtypes(include=[np.number]).columns
    df[numeric_columns] = df[numeric_columns].fillna(0)

    # Save cleaned dataset
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    cleaned_filename = f"cleaned_combined_data_{timestamp}.csv"
    df.to_csv(cleaned_filename, index=False)
    print(f"üíæ Cleaned dataset saved as: {cleaned_filename}")

    return df, cleaned_filename

# Run the combination and cleaning process
df_clean, csv_filename = load_and_combine_csvs()

if df_clean is not None:
    print(f"\nüìä Dataset ready for regex analysis:")
    print(f"  - Shape: {df_clean.shape}")
    print(f"  - Columns: {list(df_clean.columns)}")
    print(f"  - Saved as: {csv_filename}")

    # Display sample
    print("\nSample URLs:")
    print(df_clean['Full URL'].head(10).tolist())
else:
    print("‚ùå Setup failed - check your CSV files and try again")

"""## CELL 2 - Regex Pattern Library & Testing Engine (Run Repeatedly)"""

# Global pattern library - edit and expand as needed
# IMPORTANT: Patterns are processed in ORDER - put more specific patterns FIRST!

REGEX_PATTERNS = OrderedDict([
    ('404 Pages', {
        'pattern': r'(?:^https://www\.payscale\.com)?/404(?:\?.*)?$\'',
        'description': '404 Pages',
        'color': '#FF9F43',
        'priority': 0
    }),

    ('Fonts Pages', {
        'pattern': r'/fonts/|/webfonts/',
        'description': 'Fonts pages',
        'color': '#96CEB4',
        'priority': 0
    }),

    ('Htm Pages', {
        'pattern': r'(?:^https://www\.payscale\.com)?/.*\.htm(?:\?.*)?$',
        'description': 'Old pages ending with .htm.',
        'color': '#F0A3A3',
        'priority': 0
    }),

    ('Insecure URLs', {
        'pattern': r'http\:\/\/',
        'description': 'HTTP URLs',
        'color': '#87CEEB',
        'priority': 0
    }),

    ('JavaScript Void Links', {
        'pattern': r'/javascript:void',
        'description': 'JavaScript void placeholder links',
        'color': '#DDA0DD',
        'priority': 0
    }),

    ('Scripts Data', {
        'pattern': r'\.(php|xml|json|csv|txt|js|css)(\?|$)',
        'description': 'Scripts and data files',
        'color': '#B19CD9',
        'priority': 0
    }),

    ('Security Scan - Backup Files', {
        'pattern': r'\.(bak|sql|zip|7z|rar|old|tar|gz|dump)(?:\?.*)?$',
        'description': 'Security scan attempts - backup files',
        'color': '#45B7D1',
        'priority': 0
    }),

    ('Security Scan - Double Slash Errors', {
        'pattern': r'payscale\.com//[^/]',
        'description': 'URLs with double slash path errors',
        'color': '#4ECDC4',
        'priority': 0
    }),

    ('Security Scan - Malformed Domain Errors', {
        'pattern': r'https://www\.payscale\.com/https://|payscale\.com/http://|\.com/www\.',
        'description': 'Malformed URLs with protocol/domain errors',
        'color': '#FF6B6B',
        'priority': 0
    }),

    ('System Files', {
        'pattern': r'(?:/(?:crossdomain\.xml|robots\.txt|sitemap(?:\.xml|\.txt)?|feeds?(?:\.xml|\.rss)?|favicon\.ico?)(?:\?.*)?$|/sitemap/?$)',
        'description': 'System and SEO files',
        'color': '#FFEAA7',
        'priority': 0
    }),

    ('ASPX Pages', {
        'pattern': r'\.(aspx)(\?|$)',
        'description': 'ASPX Pages',
        'color': '#B19CD9',
        'priority': 1
    }),

    ('B2B - Compensation Trends', {
        'pattern': r'(?:^https://www\.payscale\.com)?/(compensation-trends).*',
        'description': 'Compesation Trend pages',
        'color': '#FFEAA7',
        'priority': 1
    }),

    ('B2B - Home', {
        'pattern': r'^(?:https://www\.payscale\.com)?(?:/(?:en-(?:eu|gb))?/?)?(?:\?.*)?$',
        'description': 'Homepage and international variants',
        'color': '#45B7D1',
        'priority': 1
    }),

    ('B2B - Partnerships', {
        'pattern': r'(?:^https://www\.payscale\.com)?/(partnerships).*',
        'description': 'Partnership pages',
        'color': '#96CEB4',
        'priority': 1
    }),

    ('B2B - Research Insights', {
        'pattern': r'(?:^https://www\.payscale\.com)?/(research-and-insights|featured-content|research-and-insight|research-and-advice).*',
        'description': 'Research insights and featured content',
        'color': '#DDA0DD',
        'priority': 1
    }),

    ('Documents', {
        'pattern': r'\.(pdf|doc|docx|xls|xlsx|ppt|pptx)(\?|$)',
        'description': 'Document files',
        'color': '#4ECDC4',
        'priority': 1
    }),

    ('Images', {
        'pattern': r'\.(jpg|jpeg|png|gif|bmp|svg|webp)(\?|$)',
        'description': 'Image files',
        'color': '#FF6B6B',
        'priority': 1
    }),

    ('Wordpress Pages', {
        'pattern': r'/(wp-content|wp-includes|wp-json)/',
        'description': 'Wordpress Specific Pages',
        'color': '#87CEEB',
        'priority': 1
    }),

    ('B2B - About', {
        'pattern': r'(?:^https://www\.payscale\.com)?/(about|careers).*',
        'description': 'About and careers pages',
        'color': '#96CEB4',
        'priority': 2
    }),

    ('B2B - Career Advice', {
        'pattern': r'(?:^https://www\.payscale\.com)?/(career-advice).*',
        'description': 'Career Advice pages',
        'color': '#96CEB4',
        'priority': 2
    }),

    ('B2B - Compensation Trends', {
        'pattern': r'(?:^https://www\.payscale\.com)?/(compensation-trends).*',
        'description': 'Compensation trends pages',
        'color': '#45B7D1',
        'priority': 2
    }),

    ('B2B - Demo Request', {
        'pattern': r'(?:^https://www\.payscale\.com)?/(hr/|demo-request).*',
        'description': 'HR and demo request pages',
        'color': '#FF9F43',
        'priority': 2
    }),

    ('B2B - Events', {
        'pattern': r'(?:^https://www\.payscale\.com)?/(compference|events).*',
        'description': 'Compference and events',
        'color': '#DDA0DD',
        'priority': 2
    }),

    ('B2B - Marketplace', {
        'pattern': r'(?:^https://www\.payscale\.com)?/(marketplace).*',
        'description': 'Marketplace pages',
        'color': '#4ECDC4',
        'priority': 2
    }),

    ('B2B - Other Blogs', {
        'pattern': r'(?:^https://www\.payscale\.com)?/(career-news|compensation-today|blog|news).*',
        'description': 'Blog posts and news articles',
        'color': '#FFEAA7',
        'priority': 2
    }),

    ('B2B - Payscale Index', {
        'pattern': r'(?:^https://www\.payscale\.com)?/(payscale-index).*',
        'description': 'Payscale Index pages',
        'color': '#FF9F43',
        'priority': 2
    }),

    ('B2B - Press News', {
        'pattern': r'(?:^https://www\.payscale\.com)?/(press-releases|in-the-news).*',
        'description': 'Press releases and news',
        'color': '#FFEAA7',
        'priority': 2
    }),

    ('B2B - Price a Job', {
        'pattern': r'(?:^https://www\.payscale\.com)?/(price-a-job|plans).*',
        'description': 'Price a job and plans',
        'color': '#B19CD9',
        'priority': 2
    }),

    ('B2B - Products', {
        'pattern': r'(?:^https://www\.payscale\.com)?/(products).*',
        'description': 'Product pages',
        'color': '#F0A3A3',
        'priority': 2
    }),

    ('B2B - Solutions', {
        'pattern': r'(?:^https://www\.payscale\.com)?/(solutions).*',
        'description': 'Solutions pages',
        'color': '#87CEEB',
        'priority': 2
    }),

    ('B2B - Thank-you', {
        'pattern': r'(?:^https://www\.payscale\.com)?/(thank-you|(.*/thanks)).*',
        'description': 'Thank you pages',
        'color': '#F0A3A3',
        'priority': 2
    }),

    ('B2B - Why Payscale', {
        'pattern': r'(?:^https://www\.payscale\.com)?/(why-payscale|customer-stories).*',
        'description': 'Why Payscale and customer stories',
        'color': '#FF6B6B',
        'priority': 2
    }),

    ('B2C - College ROI', {
        'pattern': r'(?:^https://www\.payscale\.com)?/(college-roi).*',
        'description': 'College ROI pages',
        'color': '#45B7D1',
        'priority': 2
    }),

    ('B2C - College Salary Report', {
        'pattern': r'(?:^https://www\.payscale\.com)?/(college-salary-report).*',
        'description': 'College Salary Report pages',
        'color': '#4ECDC4',
        'priority': 2
    }),

    ('B2C - Research', {
        'pattern': r'(?:^https://www\.payscale\.com)?/(research).*',
        'description': 'Research and data pages',
        'color': '#87CEEB',
        'priority': 2
    }),

    ('B2C - Tools Calculators', {
        'pattern': r'(?:^https://www\.payscale\.com)?/(cost-of-living-calculator|cost-ofliving-calculator|calculators|calculator|tool).*',
        'description': 'Interactive tools and calculators',
        'color': '#FF6B6B',
        'priority': 2
    }),

    ('B2B - Customer Success', {
        'pattern': r'(?:^https://www\.payscale\.com)?/(customer-(success|onboarding)).*',
        'description': 'Customer success and onboarding',
        'color': '#DDA0DD',
        'priority': 3
    }),

    ('Admin Pages', {
        'pattern': r'(?:admin|administrator|login|cms)',
        'description': 'Administrative pages',
        'color': '#FF6B6B',
        'priority': 4
    }),

    ('Data Package Variations', {
        'pattern': r'^https\:\/\/www\.payscale\.com/data=packages/',
        'description': 'Data package URL variations',
        'color': '#F0A3A3',
        'priority': 4
    }),

    ('Paginated Pages', {
        'pattern': r'^(?:https://www\.payscale\.com)?/page/\d+/?(?:\?.*)?$',
        'description': 'Paginated Pages',
        'color': '#4ECDC4',
        'priority': 4
    }),

    ('Search Pages', {
        'pattern': r'(?:^https://www\.payscale\.com)?/?(?:\?s=.*|\?.*&s=.*|search.*)$',
        'description': 'Search pages.',
        'color': '#B19CD9',
        'priority': 4
    }),

    ('Security Scan - CMS Systems', {
        'pattern': r'/(?:typo3|textpattern|roundcube|ispmgr)(?:/|$)',
        'description': 'Security scan attempts - CMS system probes',
        'color': '#DDA0DD',
        'priority': 4
    }),

    ('Security Scan - Config Files', {
        'pattern': r'/\.(?:env|git|local|production|remote)(?:/|$)|/(?:config|web\.config|_config|\.htaccess)(?:\.|_|$)',
        'description': 'Security scan attempts - configuration files',
        'color': '#87CEEB',
        'priority': 4
    }),

    ('Security Scan - Database Files', {
        'pattern': r'/(?:database|mysql|backup|dump|sql|db)\.',
        'description': 'Security scan attempts - database files',
        'color': '#FF6B6B',
        'priority': 4
    }),

    ('Security Scan - Encoded Attacks', {
        'pattern': r'%25[0-9A-F]{2}|%2520|%253A|%255C',
        'description': 'Security scan attempts - URL encoded attacks',
        'color': '#96CEB4',
        'priority': 4
    }),

    ('Security Scan - Malformed Duplicates', {
        'pattern': r'(?:http:/www\.payscale\.com|https:/www\.81cents\.com|"|>|<|\.\.\.)',
        'description': 'Malformed URLs with protocol errors and HTML entities',
        'color': '#B19CD9',
        'priority': 4
    }),

    ('Security Scan - Malformed Job Searches', {
        'pattern': r'/(?:Installation/SaIary|researchJob/[A-Z]{2}/=)',
        'description': 'Malformed job search attempts with typos',
        'color': '#87CEEB',
        'priority': 4
    }),

    ('Security Scan - Malformed URLs', {
        'pattern': r'(?:^https://www\.payscale\.com/[-~:,]|/[0-9]{1,3}\.(?:bak|sql|zip|old)|payscale-payscale|com-com-com|www-www-www)',
        'description': 'Security scan attempts - malformed URLs',
        'color': '#45B7D1',
        'priority': 4
    }),

    ('Security Scan - Proxy Auth', {
        'pattern': r'/_proxy_auth\?|/\+CSCOE\+/',
        'description': 'Security scan attempts - proxy authentication',
        'color': '#FFEAA7',
        'priority': 4
    }),

    ('Security Scan - Single Character Paths', {
        'pattern': r'^https://www\.payscale\.com/[_~c;]/?$',
        'description': 'Single character or special character paths',
        'color': '#FF9F43',
        'priority': 4
    }),

    ('Security Scan - System Files', {
        'pattern': r'/(?:phpinfo|phpmyadmin|adminer|wp-admin|manager|admpanel|backend|login\.php|admin\.php)',
        'description': 'Security scan attempts - admin panels',
        'color': '#4ECDC4',
        'priority': 4
    }),

    ('Security Scan - URL Encoded Paths', {
        'pattern': r'^https://www\.payscale\.com/%[0-9A-F]{2}',
        'description': 'URL encoded path attempts',
        'color': '#F0A3A3',
        'priority': 4
    }),

    ('Single Letter Paths', {
        'pattern': r'^https\:\/\/www\.payscale\.com/[a-z]/?',
        'description': 'Single letter path pages',
        'color': '#FF9F43',
        'priority': 4
    }),

])

"""## Code to Run"""

def create_pattern_match_breakdown(df, patterns_dict):
    """Create detailed breakdown of which patterns each URL matches"""
    print("üîç CREATING PATTERN MATCH BREAKDOWN...")
    print("=" * 50)

    breakdown_data = []
    total_urls = len(df)

    # Use larger increments for less console spam
    progress_increment = max(10000, total_urls // 20)  # Show progress 20 times max, or every 10k URLs

    for idx, url in enumerate(df['Full URL']):
        if idx % progress_increment == 0:
            print(f"Processing URL {idx:,} of {total_urls:,} ({(idx/total_urls)*100:.1f}%)")

        matches = []
        match_count = 0

        for pattern_name, pattern_info in patterns_dict.items():
            pattern = pattern_info['pattern']

            if re.search(pattern, url, re.IGNORECASE):
                matches.append(pattern_name)
                match_count += 1

        breakdown_data.append({
            'Full URL': url,
            'Total_Matches': match_count,
            'Matched_Patterns': '; '.join(matches) if matches else 'None',
            'Primary_Category': matches[0] if matches else 'Uncategorized'
        })

    breakdown_df = pd.DataFrame(breakdown_data)
    breakdown_df = breakdown_df.sort_values(['Total_Matches', 'Full URL'], ascending=[False, True])

    print(f"‚úÖ Pattern breakdown complete!")
    print(f"üìä Match statistics:")
    print(breakdown_df['Total_Matches'].value_counts().head(10))

    return breakdown_df

def test_and_segment_urls(df, patterns_dict, show_samples=True):
    """Test regex patterns and create segmented datasets"""
    print("üéØ REGEX PATTERN TESTING & SEGMENTATION")
    print("=" * 60)
    print("‚ö†Ô∏è  PROCESSING ORDER: Patterns are applied in order - first match wins!")

    df_segmented = df.copy()
    df_segmented['url_category'] = 'Uncategorized'
    df_segmented['matched_patterns'] = ''
    df_segmented['processing_order'] = 0

    results = {}
    total_urls = len(df)
    processing_order = 1

    # Track all pattern matches (not just first match)
    all_pattern_matches = {}

    for pattern_name, pattern_info in patterns_dict.items():
        pattern = pattern_info['pattern']

        # Find ALL URLs that match this pattern
        all_matches = []
        for url in df['Full URL']:
            if re.search(pattern, url, re.IGNORECASE):
                all_matches.append(url)

        all_pattern_matches[pattern_name] = all_matches

        # Find URLs that are still uncategorized AND match this pattern
        uncategorized_mask = df_segmented['url_category'] == 'Uncategorized'
        pattern_matches = df_segmented['Full URL'].str.contains(pattern, case=False, na=False, regex=True)

        newly_categorized = uncategorized_mask & pattern_matches
        newly_categorized_urls = df_segmented.loc[newly_categorized, 'Full URL'].tolist()

        # Update categorization for newly matched URLs
        df_segmented.loc[newly_categorized, 'url_category'] = pattern_name
        df_segmented.loc[newly_categorized, 'processing_order'] = processing_order

        # Add this pattern to matched_patterns for ALL matching URLs (not just newly categorized)
        df_segmented.loc[pattern_matches, 'matched_patterns'] += f"{pattern_name}; "

        newly_categorized_count = newly_categorized.sum()

        results[pattern_name] = {
            'total_matches': len(all_matches),
            'newly_categorized': newly_categorized_count,
            'percentage_of_total': (len(all_matches) / total_urls) * 100,
            'percentage_categorized': (newly_categorized_count / total_urls) * 100,
            'sample_matches': newly_categorized_urls[:3],  # Show only newly categorized samples
            'all_sample_matches': all_matches[:3],  # Keep all matches for reporting
            'priority': pattern_info.get('priority', 5),
            'processing_order': processing_order,
            'pattern': pattern,
            'description': pattern_info.get('description', ''),
            'newly_categorized_urls': newly_categorized_urls[:2]  # Store for markdown report
        }

        print(f"\nüìã {pattern_name} (Order: {processing_order})")
        print(f"   Pattern: {pattern}")
        print(f"   Total matches: {len(all_matches):,} ({results[pattern_name]['percentage_of_total']:.1f}%)")
        print(f"   Newly categorized: {newly_categorized_count:,} ({results[pattern_name]['percentage_categorized']:.1f}%)")

        if show_samples and newly_categorized_urls:
            print(f"   Sample newly categorized URLs:")
            for sample in newly_categorized_urls[:2]:
                print(f"     ‚Ä¢ {sample}")
        elif show_samples and newly_categorized_count == 0:
            print(f"   No new URLs categorized (all matching URLs were already categorized)")

        processing_order += 1

    df_segmented['matched_patterns'] = df_segmented['matched_patterns'].str.rstrip('; ')

    category_counts = df_segmented['url_category'].value_counts()
    uncategorized_count = category_counts.get('Uncategorized', 0)
    categorized_count = total_urls - uncategorized_count

    print(f"\nüìä SEGMENTATION SUMMARY")
    print("=" * 30)
    print(f"Total URLs: {total_urls:,}")
    print(f"Categorized: {categorized_count:,} ({(categorized_count/total_urls)*100:.1f}%)")
    print(f"Uncategorized: {uncategorized_count:,} ({(uncategorized_count/total_urls)*100:.1f}%)")

    # Add comprehensive pattern match summary
    print(f"\nüìà COMPREHENSIVE PATTERN MATCH SUMMARY")
    print("=" * 45)
    print(f"{'Pattern Name':<35} {'Total Matches':<15} {'Primary Category':<18} {'Overlap Matches':<15}")
    print("-" * 85)

    for pattern_name, pattern_info in results.items():
        total_matches = pattern_info['total_matches']
        newly_categorized = pattern_info['newly_categorized']
        overlap_matches = total_matches - newly_categorized

        print(f"{pattern_name:<35} {total_matches:<15,} {newly_categorized:<18,} {overlap_matches:<15,}")

    return df_segmented, results

def create_security_mappings_csv(df_segmented, patterns_dict, timestamp):
    """Create CSV of all URLs that match security patterns with their primary category and additional mappings"""
    print("üîí CREATING SECURITY MAPPINGS ANALYSIS...")

    # Get all security pattern names
    security_patterns = [name for name in patterns_dict.keys() if name.startswith('Security Scan')]

    if not security_patterns:
        print("‚ö†Ô∏è No security patterns found")
        return None

    security_data = []

    for idx, row in df_segmented.iterrows():
        url = row['Full URL']
        primary_category = row['url_category']
        all_matched_patterns = row['matched_patterns'].split('; ') if row['matched_patterns'] else []

        # Check if this URL matches any security patterns
        security_matches = [pattern for pattern in all_matched_patterns if pattern in security_patterns]

        if security_matches:
            # Get non-security patterns
            other_matches = [pattern for pattern in all_matched_patterns if pattern not in security_patterns]

            security_data.append({
                'Full_URL': url,
                'Primary_Category': primary_category,
                'Security_Patterns_Matched': '; '.join(security_matches),
                'Other_Patterns_Matched': '; '.join(other_matches) if other_matches else 'None',
                'Total_Security_Patterns': len(security_matches),
                'Total_Other_Patterns': len(other_matches),
                'Is_Primary_Security': primary_category in security_patterns
            })

    if not security_data:
        print("‚ö†Ô∏è No URLs found matching security patterns")
        return None

    security_df = pd.DataFrame(security_data)
    security_filename = f"security_mappings_analysis_{timestamp}.csv"
    security_df.to_csv(security_filename, index=False)

    print(f"üîí Security mappings saved: {security_filename} ({len(security_df):,} URLs)")
    print(f"   - URLs with security as primary category: {security_df['Is_Primary_Security'].sum():,}")
    print(f"   - URLs with security as secondary match: {(~security_df['Is_Primary_Security']).sum():,}")

    return security_filename

def create_summary_csvs(df_segmented, results, timestamp):
    """Create CSV files for primary category breakdown and comprehensive pattern analysis"""

    # 1. Primary Category Breakdown CSV
    category_counts = df_segmented['url_category'].value_counts()
    total_urls = len(df_segmented)

    primary_breakdown_data = []
    for category, count in category_counts.items():
        percentage = (count / total_urls) * 100
        processing_order = results.get(category, {}).get('processing_order', 'N/A')
        priority = results.get(category, {}).get('priority', 'N/A')

        primary_breakdown_data.append({
            'Category': category,
            'URL_Count': count,
            'Percentage': round(percentage, 2),
            'Processing_Order': processing_order,
            'Priority_Level': priority
        })

    primary_df = pd.DataFrame(primary_breakdown_data)
    primary_df = primary_df.sort_values('URL_Count', ascending=False)
    primary_filename = f"primary_category_breakdown_{timestamp}.csv"
    primary_df.to_csv(primary_filename, index=False)

    # 2. Comprehensive Pattern Match Analysis CSV
    pattern_analysis_data = []
    sorted_results = sorted(results.items(), key=lambda x: x[1]['processing_order'])

    for pattern_name, pattern_info in sorted_results:
        total_matches = pattern_info['total_matches']
        newly_categorized = pattern_info['newly_categorized']
        overlap_matches = total_matches - newly_categorized

        pattern_analysis_data.append({
            'Pattern_Name': pattern_name,
            'Processing_Order': pattern_info['processing_order'],
            'Priority_Level': pattern_info['priority'],
            'Pattern_Regex': pattern_info['pattern'],
            'Description': pattern_info.get('description', ''),
            'Total_Matches': total_matches,
            'Primary_Category_Count': newly_categorized,
            'Overlap_Matches': overlap_matches,
            'Total_Match_Percentage': round(pattern_info['percentage_of_total'], 2),
            'Primary_Category_Percentage': round(pattern_info['percentage_categorized'], 2)
        })

    comprehensive_df = pd.DataFrame(pattern_analysis_data)
    comprehensive_filename = f"comprehensive_pattern_analysis_{timestamp}.csv"
    comprehensive_df.to_csv(comprehensive_filename, index=False)

    print(f"üìä Summary CSVs created:")
    print(f"   - Primary category breakdown: {primary_filename}")
    print(f"   - Comprehensive pattern analysis: {comprehensive_filename}")

    return primary_filename, comprehensive_filename

def export_markdown_report(df_segmented, results, timestamp):
    """Export detailed markdown report of the regex analysis"""
    total_urls = len(df_segmented)
    category_counts = df_segmented['url_category'].value_counts()
    uncategorized_count = category_counts.get('Uncategorized', 0)
    categorized_count = total_urls - uncategorized_count

    markdown_content = f"""# PayScale URL Categorization Report
*Generated on {timestamp}*

## üìä Executive Summary

| Metric | Count | Percentage |
|--------|-------|------------|
| **Total URLs Analyzed** | {total_urls:,} | 100.0% |
| **Successfully Categorized** | {categorized_count:,} | {(categorized_count/total_urls)*100:.1f}% |
| **Remaining Uncategorized** | {uncategorized_count:,} | {(uncategorized_count/total_urls)*100:.1f}% |

## üéØ Primary Category Breakdown (First Match Wins)

### Top 15 Categories by Volume

| Rank | Category | Count | Percentage | Processing Order |
|------|----------|-------|------------|------------------|
"""

    for rank, (category, count) in enumerate(category_counts.head(15).items(), 1):
        percentage = (count / total_urls) * 100
        order = results.get(category, {}).get('processing_order', 'N/A')
        markdown_content += f"| {rank} | {category} | {count:,} | {percentage:.1f}% | #{order} |\n"

    # Add comprehensive pattern match summary
    markdown_content += f"""

## üìà Comprehensive Pattern Match Analysis

*This section shows all pattern matches, including overlaps where URLs match multiple patterns*

| Pattern Name | Total Matches | Primary Category | Overlap Matches | Total % | Primary % |
|--------------|---------------|------------------|-----------------|---------|-----------|
"""

    # Sort by processing order to match console output
    sorted_results = sorted(results.items(), key=lambda x: x[1]['processing_order'])

    for pattern_name, pattern_info in sorted_results:
        total_matches = pattern_info['total_matches']
        newly_categorized = pattern_info['newly_categorized']
        overlap_matches = total_matches - newly_categorized
        total_pct = pattern_info['percentage_of_total']
        primary_pct = pattern_info['percentage_categorized']

        markdown_content += f"| {pattern_name} | {total_matches:,} | {newly_categorized:,} | {overlap_matches:,} | {total_pct:.1f}% | {primary_pct:.1f}% |\n"

    # Add detailed pattern information with samples
    markdown_content += f"""

## üîç Detailed Pattern Analysis

*Patterns are processed in order - first match determines primary category*

"""

    for pattern_name, pattern_info in sorted_results:
        order = pattern_info['processing_order']
        pattern = pattern_info['pattern']
        description = pattern_info.get('description', '')
        total_matches = pattern_info['total_matches']
        newly_categorized = pattern_info['newly_categorized']
        newly_categorized_urls = pattern_info.get('newly_categorized_urls', [])

        markdown_content += f"""### #{order}: {pattern_name}

**Pattern:** `{pattern}`
**Description:** {description}
**Total Matches:** {total_matches:,} ({pattern_info['percentage_of_total']:.1f}%)
**Newly Categorized:** {newly_categorized:,} ({pattern_info['percentage_categorized']:.1f}%)

"""

        if newly_categorized_urls:
            markdown_content += f"**Sample Newly Categorized URLs:**\n"
            for url in newly_categorized_urls:
                markdown_content += f"- `{url}`\n"
        elif newly_categorized == 0:
            markdown_content += f"*No new URLs categorized (all matching URLs were already categorized)*\n"

        markdown_content += f"\n---\n\n"

    markdown_content += f"""
## üìã Technical Details

- **Analysis Date**: {timestamp}
- **Total Patterns**: {len(results)}
- **Processing Method**: First-match-wins sequential processing
- **Pattern Priority**: Patterns are processed in the order defined, with more specific patterns typically processed first

### Notes
- **Primary Category**: URLs counted only once based on first matching pattern
- **Total Matches**: All URLs that match the pattern, regardless of processing order
- **Overlap Matches**: URLs that match this pattern but were already categorized by an earlier pattern

---
*Report generated by PayScale URL Categorization Tool*
"""

    markdown_filename = f"url_categorization_report_{timestamp}.md"
    with open(markdown_filename, 'w', encoding='utf-8') as f:
        f.write(markdown_content)

    print(f"üìÑ Markdown report saved: {markdown_filename}")
    return markdown_filename

def export_segmented_data(df_segmented, results=None, breakdown_df=None):
    """Export categorized and uncategorized data to separate CSVs with markdown report"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # Export categorized data
    df_categorized = df_segmented[df_segmented['url_category'] != 'Uncategorized'].copy()
    categorized_filename = f"categorized_urls_{timestamp}.csv"
    df_categorized.to_csv(categorized_filename, index=False)
    print(f"üì§ Saved: {categorized_filename} ({len(df_categorized):,} rows)")

    # Export uncategorized data
    df_uncategorized = df_segmented[df_segmented['url_category'] == 'Uncategorized'].copy()
    uncategorized_filename = f"uncategorized_urls_{timestamp}.csv"
    df_uncategorized.to_csv(uncategorized_filename, index=False)
    print(f"üì§ Saved: {uncategorized_filename} ({len(df_uncategorized):,} rows)")

    # Export pattern match breakdown
    if breakdown_df is not None:
        breakdown_filename = f"pattern_match_breakdown_{timestamp}.csv"
        breakdown_df.to_csv(breakdown_filename, index=False)
        print(f"üì§ Saved: {breakdown_filename} ({len(breakdown_df):,} rows)")
        files.download(breakdown_filename)

    if len(df_uncategorized) > 0:
        files.download(uncategorized_filename)

    # Export complete dataset
    complete_filename = f"complete_segmented_urls_{timestamp}.csv"
    df_segmented.to_csv(complete_filename, index=False)
    print(f"üì§ Saved: {complete_filename} ({len(df_segmented):,} rows)")

    # **FIX 1: Create and export security mappings CSV**
    if results:
        security_filename = create_security_mappings_csv(df_segmented, REGEX_PATTERNS, timestamp)
        if security_filename:
            files.download(security_filename)

    # **FIX 2: Create and export summary CSVs**
    if results:
        primary_csv, comprehensive_csv = create_summary_csvs(df_segmented, results, timestamp)
        files.download(primary_csv)
        files.download(comprehensive_csv)

    # Export markdown report
    if results:
        try:
            markdown_filename = export_markdown_report(df_segmented, results, timestamp)
            files.download(markdown_filename)
            print(f"‚úÖ Markdown report downloaded successfully!")
        except Exception as e:
            print(f"‚ö†Ô∏è Warning: Could not download markdown report: {str(e)}")

    return categorized_filename, uncategorized_filename, complete_filename

# Run the analysis
if 'df_clean' in globals() and df_clean is not None:
    print("üöÄ Starting URL analysis...")

    # Step 1: Run main segmentation
    df_segmented, pattern_results = test_and_segment_urls(df_clean, REGEX_PATTERNS)

    # Step 2: Create detailed pattern match breakdown
    print(f"\nüîç Creating detailed pattern match breakdown...")
    breakdown_df = create_pattern_match_breakdown(df_clean, REGEX_PATTERNS)

    # Step 3: Export all data
    print(f"\nüíæ EXPORTING SEGMENTED DATA...")
    print("=" * 40)
    cat_file, uncat_file, complete_file = export_segmented_data(df_segmented, pattern_results, breakdown_df)

    print(f"\n‚úÖ Regex analysis complete!")
    print(f"üìÅ Files exported:")
    print(f"   - Main categorization: {cat_file}, {uncat_file}, {complete_file}")
    print(f"   - Pattern breakdown CSV with match counts")
    print(f"   - Markdown report")

    # Show stats
    print(f"\nüìä Pattern Match Statistics:")
    print(f"   - URLs with multiple pattern matches: {len(breakdown_df[breakdown_df['Total_Matches'] > 1]):,}")
    print(f"   - Maximum patterns matched by single URL: {breakdown_df['Total_Matches'].max()}")
    print(f"   - Average patterns per URL: {breakdown_df['Total_Matches'].mean():.1f}")

else:
    print("‚ùå No cleaned data available. Run Cell 1 first!")